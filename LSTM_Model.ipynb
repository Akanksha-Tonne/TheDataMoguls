{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nload = ['HasDetections', 'AvSigVersion', 'Census_OSVersion', 'OsBuildLab']\ndf_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv',dtype='category',usecols=load)\ndf_train['HasDetections'] = df_train['HasDetections'].astype('int8')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime, date, timedelta\n\n# AS timestamp\ndatedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy',allow_pickle=True)[()]\ndf_train['DateAS'] = df_train['AvSigVersion'].map(datedictAS)  \n\n# OS timestamp\ndatedictOS = np.load('../input/malware-timestamps-2/OSVersionTimestamps.npy',allow_pickle=True)[()]\ndf_train['DateOS'] = df_train['Census_OSVersion'].map(datedictOS)  \n\n# BL timestamp\ndef convert(x):\n    try:\n        d = datetime.strptime(x.split('.')[4],'%y%m%d-%H%M')\n    except:\n        d = np.nan\n    return d\ndf_train['DateBL'] = df_train['OsBuildLab'].map(convert)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/google-safe-browsing-transparency-report-data/data.csv')\ndata['WeekOf'] = data['WeekOf'].map(lambda x: datetime.strptime(x,'%Y-%m-%d').date())\ndatedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy',allow_pickle=True)[()]\nweekdictAS={}\nfor x in datedictAS: \n    weekdictAS[x] = (datedictAS[x] - timedelta(days= -7+1+datedictAS[x].weekday())).date()\ndf_train['WeekOf'] = df_train['AvSigVersion'].map(weekdictAS)\ndf_train = pd.merge(df_train, data, on='WeekOf', how='left')\nprint('GOOGLE DATA')\ndata.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = pd.read_csv('../input/malware-avsigversion-threats/AvSigversion_Threats.csv')\ncv = pd.DataFrame(data2.groupby('AvSigVersion')['index'].count()).rename({'index':'ThreatCount'},axis=1)\ndf_train = pd.merge(df_train,cv,on='AvSigVersion',how='left')\ndf_train['ThreatCount'].fillna(0,inplace=True)\nprint('THREAT DATA')\ndata2.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train['DateAS'], df_train['DateOS'], df_train['DateBL'], df_train['WeekOf'] \ndel df_train['AvSigVersion'], df_train['OsBuildLab'], df_train['Census_OSVersion']\nprint('TRAIN DATA')\ndf_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nload = ['HasDetections', 'AvSigVersion', 'Census_OSVersion', 'OsBuildLab']\ndf_test = pd.read_csv('../input/microsoft-malware-prediction/train.csv',dtype='category',usecols=load)\ndf_test['HasDetections'] = df_test['HasDetections'].astype('int8')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime, date, timedelta\n\n# AS timestamp\ndatedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy',allow_pickle=True)[()]\ndf_test['DateAS'] = df_test['AvSigVersion'].map(datedictAS)  \n\n# OS timestamp\ndatedictOS = np.load('../input/malware-timestamps-2/OSVersionTimestamps.npy',allow_pickle=True)[()]\ndf_test['DateOS'] = df_test['Census_OSVersion'].map(datedictOS)  \n\n# BL timestamp\ndef convert(x):\n    try:\n        d = datetime.strptime(x.split('.')[4],'%y%m%d-%H%M')\n    except:\n        d = np.nan\n    return d\ndf_test['DateBL'] = df_test['OsBuildLab'].map(convert)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/google-safe-browsing-transparency-report-data/data.csv')\ndata['WeekOf'] = data['WeekOf'].map(lambda x: datetime.strptime(x,'%Y-%m-%d').date())\ndatedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy',allow_pickle=True)[()]\nweekdictAS={}\nfor x in datedictAS: \n    weekdictAS[x] = (datedictAS[x] - timedelta(days= -7+1+datedictAS[x].weekday())).date()\ndf_test['WeekOf'] = df_test['AvSigVersion'].map(weekdictAS)\ndf_test = pd.merge(df_test, data, on='WeekOf', how='left')\nprint('GOOGLE DATA')\ndata.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = pd.read_csv('../input/malware-avsigversion-threats/AvSigversion_Threats.csv')\ncv = pd.DataFrame(data2.groupby('AvSigVersion')['index'].count()).rename({'index':'ThreatCount'},axis=1)\ndf_test = pd.merge(df_test,cv,on='AvSigVersion',how='left')\ndf_test['ThreatCount'].fillna(0,inplace=True)\nprint('THREAT DATA')\ndata2.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_test['DateAS'], df_test['DateOS'], df_test['DateBL'], df_test['WeekOf'] \ndel df_test['AvSigVersion'], df_test['OsBuildLab'], df_test['Census_OSVersion']\nprint('TEST DATA')\ndf_test.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PREPROCESSING\n\n#1) DIMENTIONALITY REDUCTION TEST DATA\n\n#based on missing value ratio\ntest_df_orig= df_test\nfiltered_col = df_test.columns[df_test.isnull().mean()>0.5]\nprint(filtered_col)\nprint(len(df_test.columns))\ndf_test=df_test.drop(list(filtered_col),axis=1)\nprint(len(df_test.columns))\n#test_df=test_df.drop(list(filtered_col),axis=1)\n\n#based on the variance of each coloumn\n#An attribute with low variance not really separating data points in anyway.so we can remove that coloumn\nl=[]\nthreshold=0.8\nfor i in df_test.columns:\n    max_rfrequency = df_test[i].value_counts(normalize=True, dropna=False).values[0]#finding maximum value of the relative frequency of values in that column\n    if(max_rfrequency>threshold):\n        l.append(i)\nprint(l)\nprint(len(l))\ndf_test=df_test.drop(l,axis=1)\nprint(len(df_test.columns))\n#test_df=test_df.drop(l,axis=1)\n\n#REPLACING MISSING VALUES IN EACH ROW \n#since majority of the columns are categorical we are replacing the missing values by mode\nfor column in df_test.columns:\n    df_test[column].fillna(df_test[column].mode()[0], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PREPROCESSING\n\n#1) DIMENTIONALITY REDUCTION TRAIN DATA\n\n#based on missing value ratio\ntrain_df_orig= df_train\nfiltered_col = df_train.columns[df_train.isnull().mean()>0.5]\nprint(filtered_col)\nprint(len(df_train.columns))\ndf_train=df_train.drop(list(filtered_col),axis=1)\nprint(len(df_train.columns))\n#test_df=test_df.drop(list(filtered_col),axis=1)\n\n#based on the variance of each coloumn\n#An attribute with low variance not really separating data points in anyway.so we can remove that coloumn\nl=[]\nthreshold=0.8\nfor i in df_train.columns:\n    max_rfrequency = df_train[i].value_counts(normalize=True, dropna=False).values[0]#finding maximum value of the relative frequency of values in that column\n    if(max_rfrequency>threshold):\n        l.append(i)\nprint(l)\nprint(len(l))\ndf_train=df_train.drop(l,axis=1)\nprint(len(df_train.columns))\n#test_df=test_df.drop(l,axis=1)\n\n#REPLACING MISSING VALUES IN EACH ROW \n#since majority of the columns are categorical we are replacing the missing values by mode\nfor column in df_train.columns:\n    df_train[column].fillna(df_train[column].mode()[0], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import sequence\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nfrom keras.optimizers import Adam\nfrom keras.models import load_model\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = df_train['HasDetections']\ndel df_train['HasDetections']\ntest_target = df_test['HasDetections']\ndel df_test['HasDetections']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df_train\ntest = df_test\nseq_len = len(df_train)\nmodel = Sequential()\nmodel.add(LSTM(256, input_shape=(seq_len, 4)))\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adam = Adam(lr=0.001)\nchk = ModelCheckpoint('best_model.pkl', monitor='val_acc', save_best_only=True, mode='max', verbose=1)\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.fit(train, train_target, epochs=200, batch_size=128, callbacks=[chk], validation_data=(test,test_target))\n#loading the model and checking accuracy on the test data\nmodel = load_model('best_model.pkl')\n\nfrom sklearn.metrics import accuracy_score\ntest_preds = model.predict_classes(test)\naccuracy_score(test_target, test_preds)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}